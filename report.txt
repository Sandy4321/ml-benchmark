




Measurements:
-------------
In order to generate plots that illustrate prediction quality over time we used the
following pragmatic approach: for VW’s SGD we measured training time for different fractions
of the data set, for Spark LR as well as Scikit-Learn for different numbers of
iterations. We ran all experiments with and without evaluation and only plotted the time elapsed in
the no-evaluation runs. As Spark does not easily allow intermediate evaluation, we had to re-re run
it with different numbers of iterations from scratch using the PEEL [11, 12] framework.







Experiments
-----------------
The overall aim of experimentation is to perform a
qualitative and quantitative analysis on the performance of
the framework proposed in this paper. Also we propose and run experiments to evaluate the performance for distributed data flow systems
for scalable machine learning workloads. We measure training time (including loading the data and writing out the model) and (in a separate run) the AuC the trained models achieve on a held-out test set. Please keep in mind that our motivation is to evaluate systems with relevant machine learning workloads along with slight deviation of machine learning algorithms.


Experiment 1: Logistic Regression as a baseline}
------------------------------------------------
In the first experiment we look at regularized Logistic Regression algorithm, a popular baseline method that can be solved using embarrassingly parallel
algorithms such as batch gradient descent. Apache Spark MLlib implements Logistic Regression
training using the Breeze library’s LBFGS solver where it locally computes partial gradient updates in parallel using all available cores and and aggregates them in the driver. As a single machine baseline we use Vowpal Wabbit which implements an online stochastic gradient decent using only two cores: one for parsing the input data and one to compute gradient updates. 

As mentioned before in the cluster hardware subsection, we executed our experiments on the big machines. One with 32 CPUs and 1TB of ram and other with 16 nodes dual quad-core cluster with 48GB RAM in total. 

Logistic regression was trained on DAS4 using our Spark solution reached AuC (test set) roughly between .76 and .82 in around 3000 to 4000 seconds in time. Apache Spark MLlib needs substantially more hardware resources and sometimes JVM failures even upon increasing the RAM to around 30 to 40 GB.   

Running LR on Big-Mem Machine it took spark for some reason much more time (3550 seconds to 4800 seconds). This, using all the core/cpu's latinum has to offer and RAM usage around ~100 to 150 GB usage reported.  
For both hardware configurations, the Spark MLlib implementation needs significantly more time to achieve comparable AuC (VW for example), even though it runs on substantially more resources. 

One of the advantages of VW as we were able to research this is that it starts immediately updating the model as data is read. Spark spends considerable time reading and caching the data, before it can run the first L-BFGS iteration. Once this is accomplished, additional iterations run very fast on the cached data. For the runs on the DAS4 it takes around 1.5 hours to 3 hours using all the nodes available. For the runs on single node we observe that Spark MLlib is slower than VW. 

Increasing number of nodes on DAS4 the performance of Spark MLlib gets within reach of VW's performance on single node.  These experiments show that even for an embarrassingly parallel learning algorithm,the latest generation distributed data flows systems such as Spark 2.2.*, which leverages explicit memory management and code-generation to exploit modern compilers and CPUs to obtain optimal performance on the JVM, need substantial hardware resources (factor 6-15) to obtain comparable prediction quality with a competent single machine implementation within the same time-frame. (Note that since VowpalWabbit is an out-of-core library both Spark MLLib and VW could be scaled to even larger data set
sizes.).





Experiment 2: Matrix Factorization using Netflix dataset}
---------------------------------------------------------
In the second experiment we used Netflix full dataset [point to section dataset]. In this exerpiment we evaluate Matrix Factorization such as either a batch version (also known as Alternating Least Squares) 
and an SGD implementation. 

Alternating Least Sequares with Spark MLlib:
--------------------------------------------
As mentioned before Spark's MLlib has the majority of all ML algorithms already implmented. One of the MLlib's algorithms is Collaborative Filtering which is 
an RDD based API. Collaborative filtering is commonly used for recommender systems. These techniques aim to fill in the missing entries of a user-item association matrix. 
spark.mllib currently supports model-based collaborative filtering, in which users and products are described by a small set of latent factors that can be used to predict missing entries.
spark.mllib uses the alternating least squares (ALS) algorithm to learn these latent factors. We trained our model with ALS. After testing our model with different values and regularzing parameter
and number of features, we achieved an RMSE of 0.99532 on the testing set with k = 3 as the number of features and lambda=0.09. The ALS model in Spark has limitions such as not allowing to set a 
different different regularizer value to the user and the item. The same lambda (symbol here) is applied in both regressions. Therefore it limits the possible range of optimization that we could reach to
minimize the RMSE. 

The figures 1 below show the RMSE in function of lambda for
k ∈ {3, 5, 7, 8}. As we can see, the smaller RMSE is obtained
with k = 3 and lambda = 0.08.


During our research we learned [14] we could implement ALS with missing values. 
At this point it started converging way faster and possbily 
much accurate compared to the privious try-outs. We also observed that low number of features
gives us the best possible score which is in
accordance with the method itself, since we aim to update the
items matrix W and the user matrix Z always one after the
other. So in between runs, each matrix update will be used in
order to compute the update of the other. The beauty of this
is that we sacrifice precision for a huge gain in computational
cost. And the best Root-Mean Squared error that we obtained
on our testing set was around 0.9798, while our best Kaggle
score was 0.98916, which was a decent score. The aim here was to compete against one framework to
another in order to get the best score.


Stochastic Gradient Descent:
-----------------------------
The other possibility is to
minimize this cost function using SGD.





Experiment 3: Linear Support Vector Machine
--------------------------------------------
The thrid and final experiment is evaulating Linear SVM on Spark MLlib.  
The linear SVM is a standard method for large-scale classification tasks. It is a linear method as described above in equation (1), 
with the loss function in the formulation given by the hinge loss:

L(w,x,y) := max{0,1 - yW^Tx}

By default, linear SVMs are trained with an L2 regularization. But Apache Spark also supports L1 regularization which in this case the problem becomes more a linear program. The linar SVMs algorithm outputs an SVM model. Given a new data point, denoted by x, the model makes predictions based on the value of w^Tx. By default , if W^T x >= 0 then the outcome is positive, and negative otherwise.

We trained our our on the criteo dataset. Using DAS4 Spark solution reached AuC on test set was roughly between .72 and .76 in around 2500 to 3720 seconds time (we ran different experiments tuning the paramaters in between each experiment also we started with days from 0-3 and building up to day 10)  

On big-mem a.k.a Latinum - the training of our program reached AuC between .73 and .78 in around 3200 seconds using all the cpu resources and ram usage between ~255 to 500 GB. This was surprisingly slower than expected, given this machine has - in terms of hardware resouces much more to offer than others. 









# References
-------------



[11] http://peel-framework.org/.

[12] C. Boden, A. Alexandrov, A. Kunft, T. Rabl, and V. Markl. Peel: A framework for benchmarking
distributed systems and algorithms. In Proceedings of the Ninth TPC Technology Conference
on Performance Evaluation and Benchmarking (TPCTC 2017) at VLDB 2017, 2017.

